# **Generalized minimal residual method (GMRES) implementation**

## **Index**

1. [**Introduction**](#introduction)
   - [**Krylov Subspace**](#krylov-subspace)
   - [**Arnoldi Iteration**](#arnoldi-iteration)
   - [**Givens Rotation**](#givens-rotation)
2. [**Generalized minimal residual method**](#generalized-minimal-residual-method)
   - [**Method Description**](#method-description)
   - [**The least squares problem**](#the-least-squares-problem)
   - [**Convergence**](#convergence)
   - [**GMRES with Restarting**](#gmres-with-restarting)
3. [**Implementation**](#implementation-info-and-comparison-with-other-methods)
4. [**Results**](#results)
5. [**Installation and virtual environment preparation**](#installation-and-virtual-environment-preparation)
6. [**Execution Guide**](#execution-guide)
7. [**References**](#references)

<hr>

## **Introduction**

In mathematics, the **generalized minimal residual method (GMRES)** is an iterative method for the numerical solution of an **indefinite non-symmetric** system of linear equations. <br>
The method approximates the solution by the vector in a [**Krylov subspace**](#krylov-subspace) with minimal residual. The **Arnoldi iteration** is used to find this vector.

#### **Krylov Subspace**
In linear algebra, the order-`r` **Krylov subspace** generated by an **n-by-n matrix** A and a vector b of dimension `n` is the linear subspace spanned by the images of b under the first r powers of A (starting from ${\displaystyle A^{0}=I}A^{0}=I)$, that is, 

$$
{\displaystyle {\mathcal {K}}_{r}(A,b)=\operatorname {span} \,\{b,Ab,A^{2}b,\ldots ,A^{r-1}b\}.}
$$

Modern iterative methods such as **Arnoldi iteration** can be used for finding one (or a few) eigenvalues of large **sparse matrices** or solving large systems of linear equations. They try to avoid matrix-matrix operations, but rather multiply vectors by the matrix and work with the resulting vectors. Starting with a vector ${\displaystyle b}$, one computes ${\displaystyle Ab}$, then one multiplies that vector by ${\displaystyle A}$ to find ${\displaystyle A^{2}b}$ and so on. All algorithms that work this way are referred to as Krylov subspace methods; they are among the most successful methods currently available in numerical linear algebra.

#### **Arnoldi Iteration**
In numerical linear algebra, the **Arnoldi iteration** is an eigenvalue algorithm and an important example of an iterative method. Arnoldi finds an approximation to the eigenvalues and eigenvectors of general (possibly non-Hermitian) matrices by constructing an orthonormal basis of the Krylov subspace, which makes it particularly useful when dealing with large sparse matrices.

The Arnoldi method belongs to a class of linear algebra algorithms that give a partial result after a small number of iterations, in contrast to so-called direct methods which must complete to give any useful results (see for example, Householder transformation). The partial result in this case being the first few vectors of the basis the algorithm is building.


#### **Givens Rotation**
In numerical linear algebra, a Givens rotation is a rotation in the plane spanned by two coordinates axes. Givens rotations are named after Wallace Givens, who introduced them to numerical analysts in the 1950s while he was working at Argonne National Laboratory.

A Givens rotation is represented by a matrix of the form

$$
{\displaystyle G(i,j,\theta )={\begin{bmatrix}1&\cdots &0&\cdots &0&\cdots &0\\\vdots &\ddots &\vdots &&\vdots &&\vdots \\0&\cdots &c&\cdots &-s&\cdots &0\\\vdots &&\vdots &\ddots &\vdots &&\vdots \\0&\cdots &s&\cdots &c&\cdots &0\\\vdots &&\vdots &&\vdots &\ddots &\vdots \\0&\cdots &0&\cdots &0&\cdots &1\end{bmatrix}},}
$$

where c = cos θ and s = sin θ appear at the intersections ith and jth rows and columns. That is, for fixed i > j, the non-zero elements of Givens matrix are given by:

$$
{\displaystyle {\begin{aligned}g_{kk}&{}=1\qquad {\text{for}}\ k\neq i,\,j\\g_{kk}&{}=c\qquad {\text{for}}\ k=i,\,j\\g_{ji}&{}=-g_{ij}=-s\\\end{aligned}}}
$$

The product G(i, j, θ)x represents a counterclockwise rotation of the vector x in the (i, j) plane of θ radians, hence the name Givens rotation.

The main use of Givens rotations in numerical linear algebra is to introduce zeros in vectors or matrices. This effect can, for example, be employed for computing the QR decomposition of a matrix. One advantage over Householder transformations is that they can easily be parallelised, and another is that often for very sparse matrices they have a lower operation count.

# **Generalized minimal residual method**
The GMRES method was developed by [Yousef Saad and Martin H. Schultz](https://doi.org/10.1137/0907058) in 1986. It is a generalization and improvement of the [MINRES method](https://doi.org/10.1137/0712047) due to Paige and Saunders in 1975. The MINRES method requires that the matrix is symmetric, but has the advantage that it only requires handling of three vectors. GMRES is a special case of the DIIS method developed by Peter Pulay in 1980. DIIS is applicable to non-linear systems.

## **Method Description**
Denote the Euclidean norm of any vector v by ${\displaystyle \|v\|}$. Denote the (square) system of linear equations to be solved by
$$
{\displaystyle Ax=b.}
$$

The matrix A is assumed to be invertible of size *m-by-m*. Furthermore, it is assumed that **b** is normalized, i.e., that ${\displaystyle \|b\|=1}$.

The *n-th* Krylov subspace for this problem is
$$
{\displaystyle K_{n}=K_{n}(A,r_{0})=\operatorname {span} \,\{r_{0},Ar_{0},A^{2}r_{0},\ldots ,A^{n-1}r_{0}\}.\,}{\displaystyle K_{n}=K_{n}(A,r_{0})=\operatorname {span} \,\{r_{0},Ar_{0},A^{2}r_{0},\ldots ,A^{n-1}r_{0}\}.\,}
$$

where ${\displaystyle r_{0}=b-Ax_{0}}$ is the initial error given an initial guess ${\displaystyle x_{0}\neq 0}$. Clearly ${\displaystyle r_{0}=b}$ if ${\displaystyle x_{0}=0}$.

GMRES approximates the exact solution of ${\displaystyle Ax=b}$ by the vector ${\displaystyle x_{n}\in K_{n}}$  that minimizes the Euclidean norm of the residual ${\displaystyle r_{n}=b-Ax_{n}}$.

The vectors ${\displaystyle r_{0},Ar_{0},\ldots A^{n-1}r_{0}}$ might be close to linearly dependent, so instead of this basis, the Arnoldi iteration is used to find orthonormal vectors ${\displaystyle q_{1},q_{2},\ldots ,q_{n}\,}$, which form a basis for ${\displaystyle K_{n}}$. In particular, ${\displaystyle q_{1}=\|r_{0}\|_{2}^{-1}r_{0}}$.

Therefore, the vector ${\displaystyle x_{n}\in K_{n}}$  can be written as ${\displaystyle x_{n}=x_{0}+Q_{n}y_{n}}$ with ${\displaystyle y_{n}\in \mathbb {R} ^{n}}$ , where ${\displaystyle Q_{n}}$  is the m-by-n matrix formed by ${\displaystyle q_{1},\ldots ,q_{n}}$ .

The Arnoldi process also produces an $({\displaystyle n+1}n+1)$-by-${\displaystyle n}$ upper Hessenberg matrix ${\displaystyle {\tilde {H}}_{n}}$ with

$$
{\displaystyle AQ_{n}=Q_{n+1}{\tilde {H}}_{n}.\,}
$$

For symmetric matrices, a symmetric tri-diagonal matrix is actually achieved, resulting in the **minres** method.

Because columns of ${\displaystyle Q_{n}}$ are orthonormal, we have

$$
{\displaystyle \|r_{n}\|=\|b-Ax_{n}\|=\|b-A(x_{0}+Q_{n}y_{n})\|=\|r_{0}-AQ_{n}y_{n}\|=\|\beta q_{1}-AQ_{n}y_{n}\|=\|\beta q_{1}-Q_{n+1}{\tilde {H}}_{n}y_{n}\|=\|Q_{n+1}(\beta e_{1}-{\tilde {H}}_{n}y_{n})\|=\|\beta e_{1}-{\tilde {H}}_{n}y_{n}\|,\,}
$$

where

$$
e_{1}=(1,0,0,\ldots ,0)^{T}\,
$$

is the first vector in the standard basis of ${\displaystyle \mathbb {R} ^{n+1}}$ , and

$$
{\displaystyle \beta =\|r_{0}\|\,,}
$$

$r_{0}$ being the first trial vector (usually zero). Hence, ${\displaystyle x_{n}}$ can be found by minimizing the Euclidean norm of the residual

$$
{\displaystyle r_{n}={\tilde {H}}_{n}y_{n}-\beta e_{1}.}
$$

This is a linear least squares problem of size n.

This yields the GMRES method. On the ${\displaystyle n}$-th iteration:

```
1. calculate q_n with the Arnoldi method;
2. find the y_n which minimizes ||r_n||;
3. compute x_n = x_0 + Q_n y_n;
4. repeat if the residual is not yet small enough
```

At every iteration, a matrix-vector product ${\displaystyle Aq_{n}}$ must be computed. This costs about ${\displaystyle 2m^{2}}$  floating-point operations for general dense matrices of size ${\displaystyle m}$, but the cost can decrease to ${\displaystyle O(m)}$ for sparse matrices. In addition to the matrix-vector product, ${\displaystyle O(nm)}$ floating-point operations must be computed at the n-th iteration.


## **The least squares problem**
One part of the GMRES method is to find the vector ${\displaystyle y_{n}}$ which minimizes

$$
{\displaystyle \|{\tilde {H}}_{n}y_{n}-\beta e_{1}\|.\,}
$$

Note that ${\displaystyle {\tilde {H}}_{n}}$ is an (n + 1)-by-n matrix, hence it gives an over-constrained linear system of n+1 equations for n unknowns.

The minimum can be computed using a QR decomposition: find an (n + 1)-by-(n + 1) orthogonal matrix Ωn and an (n + 1)-by-n upper triangular matrix ${\displaystyle {\tilde {R}}_{n}}$ such that

$$
{\displaystyle \Omega _{n}{\tilde {H}}_{n}={\tilde {R}}_{n}.}
$$

The triangular matrix has one more row than it has columns, so its bottom row consists of zero. Hence, it can be decomposed as

$$
{\tilde  {R}}_{n}={\begin{bmatrix}R_{n}\\0\end{bmatrix}},
$$

where ${\displaystyle R_{n}}$ is an n-by-n (thus square) triangular matrix.

The QR decomposition can be updated cheaply from one iteration to the next, because the Hessenberg matrices differ only by a row of zeros and a column:

$$
{\tilde  {H}}_{{n+1}}={\begin{bmatrix}{\tilde  {H}}_{n}&h_{{n+1}}\\0&h_{{n+2,n+1}}\end{bmatrix}},
$$

where $h_{n+1} = (h1_{,n+1}, …, h_{n+1,n+1})^T$. This implies that premultiplying the Hessenberg matrix with $\Omega_n$, augmented with zeroes and a row with multiplicative identity, yields almost a triangular matrix:

$$
{\begin{bmatrix}\Omega _{n}&0\\0&1\end{bmatrix}}{\tilde  {H}}_{{n+1}}={\begin{bmatrix}R_{n}&r_{{n+1}}\\0&\rho \\0&\sigma \end{bmatrix}}
$$

This would be triangular if σ is zero. To remedy this, one needs the [Givens rotation](#givens-rotation).

$$
G_{n}={\begin{bmatrix}I_{{n}}&0&0\\0&c_{n}&s_{n}\\0&-s_{n}&c_{n}\end{bmatrix}}
$$

where

$$
{\displaystyle c_{n}={\frac {\rho }{\sqrt {\rho ^{2}+\sigma ^{2}}}}\quad {{and}}\quad s_{n}={\frac {\sigma }{\sqrt {\rho ^{2}+\sigma ^{2}}}}.}
$$

With this Givens rotation, we form

$$
\Omega _{{n+1}}=G_{n}{\begin{bmatrix}\Omega _{n}&0\\0&1\end{bmatrix}}.
$$

Indeed,

$$
{\displaystyle \Omega _{n+1}{\tilde {H}}_{n+1}={\begin{bmatrix}R_{n}&r_{n+1}\\0&r_{n+1,n+1}\\0&0\end{bmatrix}}\quad {\text{with}}\quad r_{n+1,n+1}={\sqrt {\rho ^{2}+\sigma ^{2}}}}
$$

is a triangular matrix. <br>
Given the QR decomposition, the minimization problem is easily solved by noting that

$$
\|{\tilde  {H}}_{n}y_{n}-\beta e_{1}\|=\|\Omega _{n}({\tilde  {H}}_{n}y_{n}-\beta e_{1})\|=\|{\tilde  {R}}_{n}y_{n}-\beta \Omega _{n}e_{1}\|.
$$

Denoting the vector ${\displaystyle \beta \Omega _{n}e_{1}}$ by

$$
{\displaystyle {\tilde {g}}_{n}={\begin{bmatrix}g_{n}\\\gamma _{n}\end{bmatrix}}}
$$

with gn ∈ Rn and γn ∈ R, this is

$$
\|{\tilde  {H}}_{n}y_{n}-\beta e_{1}\|=\|{\tilde  {R}}_{n}y_{n}-\beta \Omega _{n}e_{1}\|=\left\|{\begin{bmatrix}R_{n}\\0\end{bmatrix}}y_{n}-{\begin{bmatrix}g_{n}\\\gamma _{n}\end{bmatrix}}\right\|.
$$

The vector y that minimizes this expression is given by

$$
{\displaystyle y_{n}=R_{n}^{-1}g_{n}.}y_{n}=R_{n}^{{-1}}g_{n}.
$$

Again, the vectors ${\displaystyle g_{n}}$ are easy to update.


## **Convergence**
One of the most important characteristics of GMRES is that it will always arrive at an exact solution (if one exists). At the $n$-th iteration, GMRES computes the best approximate solution to $Ax = b$ for $x_n$ $\in$ $K_n$. If A is full rank, then $K_m = F^m$, so the $m$-th iteration will always return an **exact answer**. <br>
Sometimes, the exact solution $x \in K_n$ for some $n < m$, in this case $x_n$ is an exact solution. In either case, the algorithm is convergent after n steps if the $n$-th residual is sufficiently small.

This does not happen in general. Indeed, a theorem of Greenbaum, Pták and Strakoš states that for every nonincreasing sequence $a_1, ..., a_{m−1}, a_m = 0$, one can find a matrix A such that the $||r_n||$ = an for all n, where $r_n$ is the residual defined above. In particular, it is possible to find a matrix for which the residual stays constant for m − 1 iterations, and only drops to zero at the last iteration.

In practice, though, GMRES often performs well. This can be proven in specific situations. If the symmetric part of A, that is ${\displaystyle (A^{T}+A)/2}$, is positive definite, then

$$
{\displaystyle \|r_{n}\|\leq \left(1-{\frac {\lambda _{\min }^{2}(1/2(A^{T}+A))}{\lambda _{\max }(A^{T}A)}}\right)^{n/2}\|r_{0}\|,}
$$

where ${\displaystyle \lambda _{\mathrm {min} }(M)}$ and ${\displaystyle \lambda _{\mathrm {max} }(M)}$ denote the smallest and largest eigenvalue of the matrix ${\displaystyle M}$, respectively.

If A is symmetric and positive definite, then we even have

$$
{\displaystyle \|r_{n}\|\leq \left({\frac {\kappa _{2}(A)^{2}-1}{\kappa _{2}(A)^{2}}}\right)^{n/2}\|r_{0}\|.}
$$

where ${\displaystyle \kappa _{2}(A)}$ denotes the condition number of A in the Euclidean norm.

In the general case, where A is not positive definite, we have

$$
{\displaystyle {\frac {\|r_{n}\|}{\|b\|}}\leq \inf _{p\in P_{n}}\|p(A)\|\leq \kappa _{2}(V)\inf _{p\in P_{n}}\max _{\lambda \in \sigma (A)}|p(\lambda )|,\,}
$$

where $P_n$ denotes the set of polynomials of degree at most n with p(0) = 1, V is the matrix appearing in the spectral decomposition of A, and $\sigma(A)$ is the spectrum of A. Roughly speaking, this says that fast convergence occurs when the eigenvalues of A are clustered away from the origin and A is not too far from normality.

All these inequalities bound only the residuals instead of the actual error, that is, the distance between the current iterate $x_n$ and the exact solution.

## **GMRES with Restart**
The frst few iterations of GMRES have low spatial and temporal complexity. However, as $k$ increases, the $k$-th iteration of GMRES becomes more expensive temporally and spatially. In fact, computing the $k$-th iteration of GMRES for very large $k$ can be prohibitively complex.

This issue is addressed by using `GMRES(k)`, or **GMRES with restarts**. When $k$ becomes large, this algorithm restarts GMRES with an improved initial guess. The new initial guess is taken to be the vector that was found upon termination of the last GMRES iteration run. The algorithm `GMRES(k)` will always have manageable spatial and temporal complexity, but it is less reliable than GMRES. If the true solution $x$ to $Ax = b$ is nearly orthogonal to the Krylov subspaces $K_n(A, b)$ for $n \le k$, then `GMRES(k)` could converge very slowly or not at all.

<hr>

## **Implementation**

<hr>

## **Installation and virtual environment preparation**


### **Execution Guide**



<hr>

### **References**
- [Wikipedia](https://en.wikipedia.org/wiki/Generalized_minimal_residual_method)
- GMRES first paper: [Yousef Saad and Martin H. Schultz](https://doi.org/10.1137/0907058)
- [GMRES lab](https://acme.byu.edu/0000017a-1bb8-db63-a97e-7bfa0bde0000/krylov2-pdf)s